{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsmPlPp3V2E4oQ9HQapKlO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxILwLCwY7Ib",
        "outputId": "f128543f-62fe-4599-d83e-08b3e7c294d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Sample sentences:\n",
            "['I love deep learning.', 'Deep learning will change the world.', 'PyTorch is a popular deep learning framework.', 'Manchester United has not been the same since Sir Alex left ', 'Why was the GPU feeling warm? It had too many layers!', 'Transformers pay more attention than I do.', \"You know you're a deep learning model when you have more parameters than friends.\"]\n",
            "\n",
            "Tokenizing the sentences...\n",
            "\n",
            "Word frequency:\n",
            "Counter({'deep': 4, 'learning': 3, 'the': 3, 'i': 2, 'a': 2, 'more': 2, 'than': 2, 'you': 2, 'love': 1, 'learning.': 1, 'will': 1, 'change': 1, 'world.': 1, 'pytorch': 1, 'is': 1, 'popular': 1, 'framework.': 1, 'manchester': 1, 'united': 1, 'has': 1, 'not': 1, 'been': 1, 'same': 1, 'since': 1, 'sir': 1, 'alex': 1, 'left': 1, 'why': 1, 'was': 1, 'gpu': 1, 'feeling': 1, 'warm?': 1, 'it': 1, 'had': 1, 'too': 1, 'many': 1, 'layers!': 1, 'transformers': 1, 'pay': 1, 'attention': 1, 'do.': 1, 'know': 1, \"you're\": 1, 'model': 1, 'when': 1, 'have': 1, 'parameters': 1, 'friends.': 1})\n",
            "\n",
            "Vocabulary with index mapping:\n",
            "{'i': 0, 'love': 1, 'deep': 2, 'learning.': 3, 'learning': 4, 'will': 5, 'change': 6, 'the': 7, 'world.': 8, 'pytorch': 9, 'is': 10, 'a': 11, 'popular': 12, 'framework.': 13, 'manchester': 14, 'united': 15, 'has': 16, 'not': 17, 'been': 18, 'same': 19, 'since': 20, 'sir': 21, 'alex': 22, 'left': 23, 'why': 24, 'was': 25, 'gpu': 26, 'feeling': 27, 'warm?': 28, 'it': 29, 'had': 30, 'too': 31, 'many': 32, 'layers!': 33, 'transformers': 34, 'pay': 35, 'more': 36, 'attention': 37, 'than': 38, 'do.': 39, 'you': 40, 'know': 41, \"you're\": 42, 'model': 43, 'when': 44, 'have': 45, 'parameters': 46, 'friends.': 47, '<pad>': 48}\n",
            "\n",
            "Tensor representations of sentences:\n",
            "[tensor([0, 1, 2, 3]), tensor([2, 4, 5, 6, 7, 8]), tensor([ 9, 10, 11, 12,  2,  4, 13]), tensor([14, 15, 16, 17, 18,  7, 19, 20, 21, 22, 23]), tensor([24, 25,  7, 26, 27, 28, 29, 30, 31, 32, 33]), tensor([34, 35, 36, 37, 38,  0, 39]), tensor([40, 41, 42, 11,  2,  4, 43, 44, 40, 45, 36, 46, 38, 47])]\n",
            "\n",
            "Preparing the dataset and dataloader...\n",
            "TextDataset:\n",
            "\n",
            "Input: [tensor([48,  0,  1,  2]), tensor([48,  2,  4,  5,  6,  7]), tensor([48,  9, 10, 11, 12,  2,  4]), tensor([48, 14, 15, 16, 17, 18,  7, 19, 20, 21, 22]), tensor([48, 24, 25,  7, 26, 27, 28, 29, 30, 31, 32]), tensor([48, 34, 35, 36, 37, 38,  0]), tensor([48, 40, 41, 42, 11,  2,  4, 43, 44, 40, 45, 36, 46, 38])]\n",
            "\n",
            "Target: [tensor([0, 1, 2, 3]), tensor([2, 4, 5, 6, 7, 8]), tensor([ 9, 10, 11, 12,  2,  4, 13]), tensor([14, 15, 16, 17, 18,  7, 19, 20, 21, 22, 23]), tensor([24, 25,  7, 26, 27, 28, 29, 30, 31, 32, 33]), tensor([34, 35, 36, 37, 38,  0, 39]), tensor([40, 41, 42, 11,  2,  4, 43, 44, 40, 45, 36, 46, 38, 47])]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"I love deep learning.\",\n",
        "    \"Deep learning will change the world.\",\n",
        "    \"PyTorch is a popular deep learning framework.\",\n",
        "    \"Manchester United has not been the same since Sir Alex left \",\n",
        "    \"Why was the GPU feeling warm? It had too many layers!\",\n",
        "    \"Transformers pay more attention than I do.\",\n",
        "    \"You know you're a deep learning model when you have more parameters than friends.\"\n",
        "]\n",
        "\n",
        "print(\"Sample sentences:\")\n",
        "print(sentences)\n",
        "print(\"\\nTokenizing the sentences...\")\n",
        "\n",
        "# Tokenizer function\n",
        "def tokenizer(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "# Create a vocabulary from the tokenized sentences\n",
        "counter = Counter()\n",
        "for sentence in sentences:\n",
        "    counter.update(tokenizer(sentence))\n",
        "\n",
        "print(\"\\nWord frequency:\")\n",
        "print(counter)\n",
        "\n",
        "# Assign each word in the vocabulary a unique index\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
        "vocab['<pad>'] = len(vocab)\n",
        "\n",
        "print(\"\\nVocabulary with index mapping:\")\n",
        "print(vocab)\n",
        "\n",
        "# Convert sentences to tensor representations\n",
        "data = [torch.tensor([vocab[word] for word in tokenizer(sentence)]) for sentence in sentences]\n",
        "\n",
        "print(\"\\nTensor representations of sentences:\")\n",
        "print(data)\n",
        "\n",
        "# Create a dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.input = [torch.cat([torch.tensor([vocab['<pad>']]), item[:-1]]) for item in data]\n",
        "        self.target = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input[idx], self.target[idx]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"TextDataset:\\n\\nInput: {self.input}\\n\\nTarget: {self.target}\"\n",
        "\n",
        "print(\"\\nPreparing the dataset and dataloader...\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    return pad_sequence(inputs, padding_value=vocab['<pad>']), pad_sequence(targets, padding_value=vocab['<pad>'])\n",
        "\n",
        "dataset = TextDataset(data)\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "21Yj5Mrze1--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNNModel Class Explained\n",
        "\n",
        "Class Declaration\n",
        "\n",
        "    RNNModel inherits from nn.Module, which is the base class for all neural network modules in PyTorch.\n",
        "\n",
        "Initialization Method (__init__)\n",
        "\n",
        "    Takes three parameters:\n",
        "        vocab_size: The number of unique words/tokens in your vocabulary.\n",
        "        embed_size: The dimensionality of the embedding vector for each word/token.\n",
        "        hidden_size: The size of the hidden state of the RNN.\n",
        "\n",
        "Embedding Layer\n",
        "\n",
        "    An embedding layer is created to convert input token IDs (usually integers) into dense vectors.\n",
        "    This allows the model to learn meaningful representations for each word/token in the vocabulary.\n",
        "\n",
        "RNN Layer\n",
        "\n",
        "    A basic RNN layer is initialized.\n",
        "    It takes the embedded input and processes it sequentially, producing a sequence of hidden states.\n",
        "    The batch_first=True argument means that input and output tensors are expected to be in the format: (batch_size, sequence_length, feature_size).\n",
        "\n",
        "Fully Connected Layer (Linear Layer)\n",
        "\n",
        "    This layer is used to map the RNN's hidden state to the vocabulary size.\n",
        "    It essentially helps in predicting the next word/token based on the RNN's hidden state.\n",
        "\n",
        "Forward Method\n",
        "\n",
        "    Defines how the input will be processed as it goes through the network.\n",
        "\n",
        "Embedding Transformation\n",
        "\n",
        "    The input sequences are transformed into dense vectors using the embedding layer.\n",
        "\n",
        "RNN Transformation\n",
        "\n",
        "    The embedded sequences are then processed by the RNN layer.\n",
        "    The RNN produces a sequence of hidden states for each time step in the input.\n",
        "    The final hidden state, which is not used in this model, is discarded.\n",
        "\n",
        "Fully Connected Layer Transformation\n",
        "\n",
        "    The RNN's output (hidden states for each time step) is passed through the fully connected layer.\n",
        "    This layer produces predictions for the next word/token for each time step in the sequence."
      ],
      "metadata": {
        "id": "HYNxJ_WJh7k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embedding(x)\n",
        "        output, _ = self.lstm(embed)\n",
        "        return self.fc(output)"
      ],
      "metadata": {
        "id": "NyjkguP_e3Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTMModel Class Explained\n",
        "\n",
        "Initialization Method (__init__)\n",
        "\n",
        "    Takes three parameters:\n",
        "        vocab_size: The number of unique words/tokens in your vocabulary.\n",
        "        embed_size: The dimensionality of the embedding vector for each word/token.\n",
        "        hidden_size: The size of the hidden state of the LSTM.\n",
        "\n",
        "Embedding Layer\n",
        "\n",
        "    An embedding layer is created to convert input token IDs (usually integers) into dense vectors.\n",
        "    This allows the model to learn meaningful representations for each word/token in the vocabulary.\n",
        "\n",
        "LSTM Layer\n",
        "\n",
        "    An LSTM (Long Short-Term Memory) layer is initialized.\n",
        "    LSTM is a type of recurrent neural network (RNN) that is capable of learning and remembering over long sequences and is less susceptible to the vanishing gradient problem compared to basic RNNs.\n",
        "    It takes the embedded input and processes it sequentially, producing a sequence of hidden states.\n",
        "\n",
        "Fully Connected Layer (Linear Layer)\n",
        "\n",
        "    This layer is used to map the LSTM's hidden state to the vocabulary size.\n",
        "    It helps in predicting the next word/token based on the LSTM's hidden state.\n",
        "\n",
        "Forward Method\n",
        "\n",
        "    Defines how the input will be processed as it goes through the network.\n",
        "\n",
        "Embedding Transformation\n",
        "\n",
        "    The input sequences are transformed into dense vectors using the embedding layer.\n",
        "\n",
        "LSTM Transformation\n",
        "\n",
        "    The embedded sequences are then processed by the LSTM layer.\n",
        "    The LSTM produces a sequence of hidden states for each time step in the input.\n",
        "    The second output from the LSTM (represented by _ in the code) includes the final hidden state and cell state, but this isn't used in the model's current configuration.\n",
        "\n",
        "Fully Connected Layer Transformation\n",
        "\n",
        "    The LSTM's output (hidden states for each time step) is passed through the fully connected layer.\n",
        "    This layer produces predictions for the next word/token for each time step in the sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "17j3rM_YiqCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.gru(x)\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "Nobkp89he_Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRUModel Class Explained\n",
        "\n",
        "Initialization Method (__init__)\n",
        "\n",
        "    Parameters:\n",
        "        vocab_size: The number of unique words/tokens in your vocabulary.\n",
        "        embed_size: The dimensionality of the embedding vector for each word/token.\n",
        "        hidden_size: The size of the hidden state of the GRU.\n",
        "\n",
        "Embedding Layer\n",
        "\n",
        "    An embedding layer is initialized, designed to convert input token IDs (typically integers) into dense vectors.\n",
        "    The layer allows the model to assign and adjust vector representations for each word/token in the vocabulary, thereby learning context and meaning.\n",
        "\n",
        "GRU Layer\n",
        "\n",
        "    A GRU (Gated Recurrent Unit) layer is initialized.\n",
        "    The GRU is a variant of a recurrent neural network (RNN). It's designed to remember past information and is typically faster and simpler than its counterpart, the LSTM, as it uses fewer gates.\n",
        "    The batch_first=True parameter means that the input tensor to the GRU should have its first dimension represent the batch size.\n",
        "\n",
        "Fully Connected Layer (Linear Layer)\n",
        "\n",
        "    A linear layer (or fully connected layer) is used to map the GRU's hidden state to a size equivalent to the vocabulary.\n",
        "    This transformation facilitates the prediction of the next word/token based on the current hidden state from the GRU.\n",
        "\n",
        "Forward Method\n",
        "\n",
        "    Describes the pathway of the input as it traverses through the network.\n",
        "\n",
        "Embedding Transformation\n",
        "\n",
        "    The input sequences, comprised of token IDs, are mapped to dense vectors via the embedding layer.\n",
        "\n",
        "GRU Transformation\n",
        "\n",
        "    The transformed embedded sequences are processed by the GRU layer.\n",
        "    The GRU yields a sequence of hidden states corresponding to each time step in the input.\n",
        "    The second output (represented by _ in the code) is the final hidden state from the GRU, which isn't employed in the current setup of the model.\n",
        "\n",
        "Fully Connected Layer Transformation\n",
        "\n",
        "    The output from the GRU, which is a series of hidden states for each time step, is routed through the fully connected layer.\n",
        "    This layer's output provides predictions for the probable next word/token for each sequence time step."
      ],
      "metadata": {
        "id": "XeFc9ViAjdfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_encoder_layers, num_decoder_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Using the nn.Transformer module which handles the full transformer architecture\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(1, 0, 2)  # Transformers expect (S, N, E) format, where S is the source sequence length, N is the batch size, and E is the embedding dimension\n",
        "        output = self.transformer(x, x, src_key_padding_mask=attention_mask, tgt_key_padding_mask=attention_mask)  # Encoder input and Decoder input are the same for our task\n",
        "        output = self.fc(output)\n",
        "        return output.permute(1, 2, 0)  # Convert back to (N, C, S) format"
      ],
      "metadata": {
        "id": "gKxrLztyfGQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TransformerModel Class Explained\n",
        "\n",
        "Initialization Method (__init__)\n",
        "\n",
        "    Parameters:\n",
        "        vocab_size: Number of unique words/tokens in the vocabulary.\n",
        "        embed_size: Dimensionality of the embedding vector for each word/token.\n",
        "        num_heads: Number of attention heads in the multi-head attention mechanism.\n",
        "        num_encoder_layers: Number of layers in the transformer encoder.\n",
        "        num_decoder_layers: Number of layers in the transformer decoder.\n",
        "\n",
        "Embedding Layer\n",
        "\n",
        "    Embedding Layer is introduced to convert the input token IDs into dense vectors.\n",
        "        This layer learns a dense representation for each word/token in the vocabulary during training.\n",
        "\n",
        "Transformer Layer\n",
        "\n",
        "    The main Transformer module from PyTorch is utilized, which provides the full architecture of the transformer, including both encoder and decoder.\n",
        "        d_model: Refers to the depth (size) of the representation, which matches the embedding size.\n",
        "        nhead: Specifies the number of heads in the multi-head attention mechanism.\n",
        "        num_encoder_layers: Specifies how many encoder layers the transformer should have.\n",
        "        num_decoder_layers: Specifies the number of decoder layers.\n",
        "\n",
        "Fully Connected Layer (Linear Layer)\n",
        "\n",
        "    A linear layer is designed to map the transformer's output to the vocabulary size, effectively allowing us to make word/token predictions based on the transformer's output.\n",
        "\n",
        "Forward Method\n",
        "\n",
        "    Parameters:\n",
        "        x: The input sequences.\n",
        "        attention_mask: Optional mask to avoid attending to specific positions.\n",
        "\n",
        "Embedding Transformation\n",
        "\n",
        "    The input sequence x is mapped to dense vectors using the embedding layer.\n",
        "\n",
        "Permutation of Dimensions\n",
        "\n",
        "    The dimensions of the input are permuted from (N, S, E) to (S, N, E) using the .permute method.\n",
        "        Reason: The transformer model in PyTorch expects the input in the format (S, N, E), where S is the source sequence length, N is the batch size, and E is the embedding dimension.\n",
        "\n",
        "Transformer Transformation\n",
        "\n",
        "    The transformer takes in the embedded sequences for both the encoder and decoder (they are the same in this case) and produces the output sequences.\n",
        "        src_key_padding_mask and tgt_key_padding_mask are optional masks to prevent the attention mechanism from focusing on specific positions (like padding tokens).\n",
        "\n",
        "Fully Connected Layer Transformation\n",
        "\n",
        "    The output from the transformer is passed through the fully connected layer to produce predictions for the next word/token for each sequence position.\n",
        "\n",
        "Output Permutation\n",
        "\n",
        "    The output dimensions are permuted back to (N, C, S) format, where N is the batch size, C is the number of channels (equal to the vocabulary size in this case), and S is the source sequence length."
      ],
      "metadata": {
        "id": "fZZGCRKrj_pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, loader, optimizer, criterion, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for text, label in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(text)\n",
        "\n",
        "            # Using reshape instead of view\n",
        "            loss = criterion(output.reshape(-1, len(vocab)), label.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch: {epoch + 1}, Loss: {total_loss / len(loader)}\")"
      ],
      "metadata": {
        "id": "M3BElL1mfSzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the RNN model as an example\n",
        "rnn = RNNModel(len(vocab), 64, 128)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
        "train_model(rnn, loader, optimizer, criterion, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm7Ize_1fbx-",
        "outputId": "7766d863-7e1e-46ed-e476-5b0c1178fa9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 3.8196704983711243\n",
            "Epoch: 2, Loss: 3.6049699187278748\n",
            "Epoch: 3, Loss: 3.316157341003418\n",
            "Epoch: 4, Loss: 3.1795292496681213\n",
            "Epoch: 5, Loss: 3.1583635807037354\n",
            "Epoch: 6, Loss: 2.879136562347412\n",
            "Epoch: 7, Loss: 2.614549160003662\n",
            "Epoch: 8, Loss: 2.4817424416542053\n",
            "Epoch: 9, Loss: 2.4807032346725464\n",
            "Epoch: 10, Loss: 2.5066662430763245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "transformer = TransformerModel(len(vocab), 64, 2, 2, 2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)\n",
        "train_model(transformer, loader, optimizer, criterion, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1kbtA5icm-q",
        "outputId": "f0ad4d99-d24a-4d98-c004-53b238bdc5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 4.130754113197327\n",
            "Epoch: 2, Loss: 4.117112815380096\n",
            "Epoch: 3, Loss: 3.763562858104706\n",
            "Epoch: 4, Loss: 3.804576098918915\n",
            "Epoch: 5, Loss: 3.5726057291030884\n",
            "Epoch: 6, Loss: 3.372740387916565\n",
            "Epoch: 7, Loss: 3.4960185289382935\n",
            "Epoch: 8, Loss: 3.3459811210632324\n",
            "Epoch: 9, Loss: 3.650835692882538\n",
            "Epoch: 10, Loss: 3.5030356645584106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = LSTMModel(len(vocab), 64, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
        "train_model(lstm, loader, optimizer, criterion, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4T0ObrVbPDR",
        "outputId": "b61e67ea-888d-4987-b885-7f69d71552f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 3.8587138056755066\n",
            "Epoch: 2, Loss: 3.7510794401168823\n",
            "Epoch: 3, Loss: 3.533338189125061\n",
            "Epoch: 4, Loss: 3.5360676646232605\n",
            "Epoch: 5, Loss: 3.333391845226288\n",
            "Epoch: 6, Loss: 3.1898970007896423\n",
            "Epoch: 7, Loss: 3.0349520444869995\n",
            "Epoch: 8, Loss: 3.136695981025696\n",
            "Epoch: 9, Loss: 2.7177299857139587\n",
            "Epoch: 10, Loss: 2.810640811920166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru = GRUModel(len(vocab), 64, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru.parameters(), lr=0.001)\n",
        "train_model(gru, loader, optimizer, criterion, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu4gAEoYbcOg",
        "outputId": "f0e977e3-53d3-472f-b172-399d93a64990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 3.894600749015808\n",
            "Epoch: 2, Loss: 3.750489056110382\n",
            "Epoch: 3, Loss: 3.670069992542267\n",
            "Epoch: 4, Loss: 3.5554893612861633\n",
            "Epoch: 5, Loss: 3.4568702578544617\n",
            "Epoch: 6, Loss: 3.3212966918945312\n",
            "Epoch: 7, Loss: 3.3148428201675415\n",
            "Epoch: 8, Loss: 3.251722037792206\n",
            "Epoch: 9, Loss: 3.053061604499817\n",
            "Epoch: 10, Loss: 2.8187662959098816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_next_word(model, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(sentence.lower())  # Convert the sentence to lowercase\n",
        "    indices = [vocab[token] for token in tokens]\n",
        "    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "    predicted_index = output[0, -1].argmax(dim=0).item()\n",
        "    return list(vocab.keys())[list(vocab.values()).index(predicted_index)]\n",
        "\n",
        "\n",
        "sentence = \"Manchester United\"\n",
        "print(f\"RNN prediction: {sentence} {infer_next_word(rnn, sentence)}\")\n",
        "print(f\"LSTM prediction: {sentence} {infer_next_word(lstm, sentence)}\")\n",
        "print(f\"GRU prediction: {sentence} {infer_next_word(gru, sentence)}\")\n",
        "print(f\"Transformer prediction: {sentence} {infer_next_word(transformer, sentence)}\")\n",
        "\n",
        "# We cant handle unseen words :)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvaCDy53Z6it",
        "outputId": "719f29b8-dcdd-494f-ecdd-e02248a5ac05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN prediction: Manchester United has\n",
            "LSTM prediction: Manchester United has\n",
            "GRU prediction: Manchester United has\n",
            "Transformer prediction: Manchester United i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qk_hZCDucHvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}