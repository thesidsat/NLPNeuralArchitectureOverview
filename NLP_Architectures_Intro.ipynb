{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy8TMtGVeAvpP9tUyAY4Tv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thesidsat/NLPNeuralArchitectureOverview/blob/main/NLP_Architectures_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxILwLCwY7Ib",
        "outputId": "f128543f-62fe-4599-d83e-08b3e7c294d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Sample sentences:\n",
            "['I love deep learning.', 'Deep learning will change the world.', 'PyTorch is a popular deep learning framework.', 'Manchester United has not been the same since Sir Alex left ', 'Why was the GPU feeling warm? It had too many layers!', 'Transformers pay more attention than I do.', \"You know you're a deep learning model when you have more parameters than friends.\"]\n",
            "\n",
            "Tokenizing the sentences...\n",
            "\n",
            "Word frequency:\n",
            "Counter({'deep': 4, 'learning': 3, 'the': 3, 'i': 2, 'a': 2, 'more': 2, 'than': 2, 'you': 2, 'love': 1, 'learning.': 1, 'will': 1, 'change': 1, 'world.': 1, 'pytorch': 1, 'is': 1, 'popular': 1, 'framework.': 1, 'manchester': 1, 'united': 1, 'has': 1, 'not': 1, 'been': 1, 'same': 1, 'since': 1, 'sir': 1, 'alex': 1, 'left': 1, 'why': 1, 'was': 1, 'gpu': 1, 'feeling': 1, 'warm?': 1, 'it': 1, 'had': 1, 'too': 1, 'many': 1, 'layers!': 1, 'transformers': 1, 'pay': 1, 'attention': 1, 'do.': 1, 'know': 1, \"you're\": 1, 'model': 1, 'when': 1, 'have': 1, 'parameters': 1, 'friends.': 1})\n",
            "\n",
            "Vocabulary with index mapping:\n",
            "{'i': 0, 'love': 1, 'deep': 2, 'learning.': 3, 'learning': 4, 'will': 5, 'change': 6, 'the': 7, 'world.': 8, 'pytorch': 9, 'is': 10, 'a': 11, 'popular': 12, 'framework.': 13, 'manchester': 14, 'united': 15, 'has': 16, 'not': 17, 'been': 18, 'same': 19, 'since': 20, 'sir': 21, 'alex': 22, 'left': 23, 'why': 24, 'was': 25, 'gpu': 26, 'feeling': 27, 'warm?': 28, 'it': 29, 'had': 30, 'too': 31, 'many': 32, 'layers!': 33, 'transformers': 34, 'pay': 35, 'more': 36, 'attention': 37, 'than': 38, 'do.': 39, 'you': 40, 'know': 41, \"you're\": 42, 'model': 43, 'when': 44, 'have': 45, 'parameters': 46, 'friends.': 47, '<pad>': 48}\n",
            "\n",
            "Tensor representations of sentences:\n",
            "[tensor([0, 1, 2, 3]), tensor([2, 4, 5, 6, 7, 8]), tensor([ 9, 10, 11, 12,  2,  4, 13]), tensor([14, 15, 16, 17, 18,  7, 19, 20, 21, 22, 23]), tensor([24, 25,  7, 26, 27, 28, 29, 30, 31, 32, 33]), tensor([34, 35, 36, 37, 38,  0, 39]), tensor([40, 41, 42, 11,  2,  4, 43, 44, 40, 45, 36, 46, 38, 47])]\n",
            "\n",
            "Preparing the dataset and dataloader...\n",
            "TextDataset:\n",
            "\n",
            "Input: [tensor([48,  0,  1,  2]), tensor([48,  2,  4,  5,  6,  7]), tensor([48,  9, 10, 11, 12,  2,  4]), tensor([48, 14, 15, 16, 17, 18,  7, 19, 20, 21, 22]), tensor([48, 24, 25,  7, 26, 27, 28, 29, 30, 31, 32]), tensor([48, 34, 35, 36, 37, 38,  0]), tensor([48, 40, 41, 42, 11,  2,  4, 43, 44, 40, 45, 36, 46, 38])]\n",
            "\n",
            "Target: [tensor([0, 1, 2, 3]), tensor([2, 4, 5, 6, 7, 8]), tensor([ 9, 10, 11, 12,  2,  4, 13]), tensor([14, 15, 16, 17, 18,  7, 19, 20, 21, 22, 23]), tensor([24, 25,  7, 26, 27, 28, 29, 30, 31, 32, 33]), tensor([34, 35, 36, 37, 38,  0, 39]), tensor([40, 41, 42, 11,  2,  4, 43, 44, 40, 45, 36, 46, 38, 47])]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"I love deep learning.\",\n",
        "    \"Deep learning will change the world.\",\n",
        "    \"PyTorch is a popular deep learning framework.\",\n",
        "    \"Manchester United has not been the same since Sir Alex left \",\n",
        "    \"Why was the GPU feeling warm? It had too many layers!\",\n",
        "    \"Transformers pay more attention than I do.\",\n",
        "    \"You know you're a deep learning model when you have more parameters than friends.\"\n",
        "]\n",
        "\n",
        "print(\"Sample sentences:\")\n",
        "print(sentences)\n",
        "print(\"\\nTokenizing the sentences...\")\n",
        "\n",
        "# Tokenizer function\n",
        "def tokenizer(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "# Create a vocabulary from the tokenized sentences\n",
        "counter = Counter()\n",
        "for sentence in sentences:\n",
        "    counter.update(tokenizer(sentence))\n",
        "\n",
        "print(\"\\nWord frequency:\")\n",
        "print(counter)\n",
        "\n",
        "# Assign each word in the vocabulary a unique index\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
        "vocab['<pad>'] = len(vocab)\n",
        "\n",
        "print(\"\\nVocabulary with index mapping:\")\n",
        "print(vocab)\n",
        "\n",
        "# Convert sentences to tensor representations\n",
        "data = [torch.tensor([vocab[word] for word in tokenizer(sentence)]) for sentence in sentences]\n",
        "\n",
        "print(\"\\nTensor representations of sentences:\")\n",
        "print(data)\n",
        "\n",
        "# Create a dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.input = [torch.cat([torch.tensor([vocab['<pad>']]), item[:-1]]) for item in data]\n",
        "        self.target = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input[idx], self.target[idx]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"TextDataset:\\n\\nInput: {self.input}\\n\\nTarget: {self.target}\"\n",
        "\n",
        "print(\"\\nPreparing the dataset and dataloader...\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    return pad_sequence(inputs, padding_value=vocab['<pad>']), pad_sequence(targets, padding_value=vocab['<pad>'])\n",
        "\n",
        "dataset = TextDataset(data)\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "21Yj5Mrze1--"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embedding(x)\n",
        "        output, _ = self.lstm(embed)\n",
        "        return self.fc(output)"
      ],
      "metadata": {
        "id": "NyjkguP_e3Qy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.gru(x)\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "Nobkp89he_Qw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_encoder_layers, num_decoder_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Using the nn.Transformer module which handles the full transformer architecture\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(1, 0, 2)  # Transformers expect (S, N, E) format, where S is the source sequence length, N is the batch size, and E is the embedding dimension\n",
        "        output = self.transformer(x, x, src_key_padding_mask=attention_mask, tgt_key_padding_mask=attention_mask)  # Encoder input and Decoder input are the same for our task\n",
        "        output = self.fc(output)\n",
        "        return output.permute(1, 2, 0)  # Convert back to (N, C, S) format"
      ],
      "metadata": {
        "id": "gKxrLztyfGQ7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, loader, optimizer, criterion, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for text, label in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(text)\n",
        "\n",
        "            # Using reshape instead of view\n",
        "            loss = criterion(output.reshape(-1, len(vocab)), label.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch: {epoch + 1}, Loss: {total_loss / len(loader)}\")"
      ],
      "metadata": {
        "id": "M3BElL1mfSzD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the RNN model as an example\n",
        "rnn = RNNModel(len(vocab), 64, 128)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
        "train_model(rnn, loader, optimizer, criterion, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm7Ize_1fbx-",
        "outputId": "7766d863-7e1e-46ed-e476-5b0c1178fa9b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 3.8196704983711243\n",
            "Epoch: 2, Loss: 3.6049699187278748\n",
            "Epoch: 3, Loss: 3.316157341003418\n",
            "Epoch: 4, Loss: 3.1795292496681213\n",
            "Epoch: 5, Loss: 3.1583635807037354\n",
            "Epoch: 6, Loss: 2.879136562347412\n",
            "Epoch: 7, Loss: 2.614549160003662\n",
            "Epoch: 8, Loss: 2.4817424416542053\n",
            "Epoch: 9, Loss: 2.4807032346725464\n",
            "Epoch: 10, Loss: 2.5066662430763245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "transformer = TransformerModel(len(vocab), 64, 2, 2, 2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)\n",
        "train_model(transformer, loader, optimizer, criterion, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1kbtA5icm-q",
        "outputId": "f0ad4d99-d24a-4d98-c004-53b238bdc5e3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 4.130754113197327\n",
            "Epoch: 2, Loss: 4.117112815380096\n",
            "Epoch: 3, Loss: 3.763562858104706\n",
            "Epoch: 4, Loss: 3.804576098918915\n",
            "Epoch: 5, Loss: 3.5726057291030884\n",
            "Epoch: 6, Loss: 3.372740387916565\n",
            "Epoch: 7, Loss: 3.4960185289382935\n",
            "Epoch: 8, Loss: 3.3459811210632324\n",
            "Epoch: 9, Loss: 3.650835692882538\n",
            "Epoch: 10, Loss: 3.5030356645584106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = LSTMModel(len(vocab), 64, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
        "train_model(lstm, loader, optimizer, criterion, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4T0ObrVbPDR",
        "outputId": "b61e67ea-888d-4987-b885-7f69d71552f5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 3.8587138056755066\n",
            "Epoch: 2, Loss: 3.7510794401168823\n",
            "Epoch: 3, Loss: 3.533338189125061\n",
            "Epoch: 4, Loss: 3.5360676646232605\n",
            "Epoch: 5, Loss: 3.333391845226288\n",
            "Epoch: 6, Loss: 3.1898970007896423\n",
            "Epoch: 7, Loss: 3.0349520444869995\n",
            "Epoch: 8, Loss: 3.136695981025696\n",
            "Epoch: 9, Loss: 2.7177299857139587\n",
            "Epoch: 10, Loss: 2.810640811920166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru = GRUModel(len(vocab), 64, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru.parameters(), lr=0.001)\n",
        "train_model(gru, loader, optimizer, criterion, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu4gAEoYbcOg",
        "outputId": "f0e977e3-53d3-472f-b172-399d93a64990"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 3.894600749015808\n",
            "Epoch: 2, Loss: 3.750489056110382\n",
            "Epoch: 3, Loss: 3.670069992542267\n",
            "Epoch: 4, Loss: 3.5554893612861633\n",
            "Epoch: 5, Loss: 3.4568702578544617\n",
            "Epoch: 6, Loss: 3.3212966918945312\n",
            "Epoch: 7, Loss: 3.3148428201675415\n",
            "Epoch: 8, Loss: 3.251722037792206\n",
            "Epoch: 9, Loss: 3.053061604499817\n",
            "Epoch: 10, Loss: 2.8187662959098816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_next_word(model, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(sentence.lower())  # Convert the sentence to lowercase\n",
        "    indices = [vocab[token] for token in tokens]\n",
        "    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "    predicted_index = output[0, -1].argmax(dim=0).item()\n",
        "    return list(vocab.keys())[list(vocab.values()).index(predicted_index)]\n",
        "\n",
        "\n",
        "sentence = \"Manchester United\"\n",
        "print(f\"RNN prediction: {sentence} {infer_next_word(rnn, sentence)}\")\n",
        "print(f\"LSTM prediction: {sentence} {infer_next_word(lstm, sentence)}\")\n",
        "print(f\"GRU prediction: {sentence} {infer_next_word(gru, sentence)}\")\n",
        "print(f\"Transformer prediction: {sentence} {infer_next_word(transformer, sentence)}\")\n",
        "\n",
        "# We cant handle unseen words :)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvaCDy53Z6it",
        "outputId": "719f29b8-dcdd-494f-ecdd-e02248a5ac05"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN prediction: Manchester United has\n",
            "LSTM prediction: Manchester United has\n",
            "GRU prediction: Manchester United has\n",
            "Transformer prediction: Manchester United i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qk_hZCDucHvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}